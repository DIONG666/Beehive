"""
æ–‡æœ¬æ‘˜è¦å™¨ï¼šé•¿æ–‡æ¡£æˆªæ–­+å‹ç¼©
"""
import re
from typing import Dict, Any, List, Optional
from config import Config


class SummarizerTool:
    """æ–‡æœ¬æ‘˜è¦å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ‘˜è¦å·¥å…·"""
        self.enabled = Config.ENABLE_SUMMARIZER
        self.max_length = Config.MAX_CONTEXT_LENGTH
        self.planner = None
        self.llm_client = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ç”¨äºæ‘˜è¦"""
        try:
            from openai import OpenAI
            self.llm_client = OpenAI(
                api_key=Config.DEEPSEEK_API_KEY,
                base_url=Config.DEEPSEEK_BASE_URL
            )
        except ImportError as e:
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ - {e}")
            print("å°†ä½¿ç”¨åŸºç¡€æ‘˜è¦åŠŸèƒ½")
    
    def summarize(self, text: str, max_length: Optional[int] = None, 
                       style: str = "general") -> Dict[str, Any]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œæ‘˜è¦
        
        Args:
            text: å¾…æ‘˜è¦çš„æ–‡æœ¬
            max_length: æœ€å¤§æ‘˜è¦é•¿åº¦
            style: æ‘˜è¦é£æ ¼ï¼ˆgeneral/academic/news/bullet_pointsï¼‰
            
        Returns:
            æ‘˜è¦ç»“æœå­—å…¸
        """
        if not text or not text.strip():
            return {
                'summary': '',
                'original_length': 0,
                'summary_length': 0,
                'compression_ratio': 0,
                'method': 'empty',
                'error': 'è¾“å…¥æ–‡æœ¬ä¸ºç©º'
            }
        
        try:
            print(f"ğŸ“ å¼€å§‹æ‘˜è¦ï¼ŒåŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
            
            max_len = max_length
            
            # å¦‚æœæ–‡æœ¬å·²ç»è¶³å¤ŸçŸ­ï¼Œç›´æ¥è¿”å›
            if len(text) <= max_len:
                return {
                    'summary': text,
                    'original_length': len(text),
                    'summary_length': len(text),
                    'compression_ratio': 1.0,
                    'method': 'no_compression',
                    'error': None
                }
            
            # æ ¹æ®æ˜¯å¦æœ‰LLMé€‰æ‹©æ‘˜è¦æ–¹æ³•
            if self.llm_client:
                summary = self._llm_summarize(text, max_len, style)
                method = 'llm'
            else:
                summary = self._extractive_summarize(text, max_len)
                method = 'extractive'
            
            compression_ratio = len(summary) / len(text)
            
            print(f"âœ… æ‘˜è¦å®Œæˆï¼Œå‹ç¼©æ¯”: {compression_ratio:.2f}")
            
            return {
                'summary': summary,
                'original_length': len(text),
                'summary_length': len(summary),
                'compression_ratio': compression_ratio,
                'method': method,
                'error': None
            }
            
        except Exception as e:
            print(f"âŒ æ‘˜è¦ç”Ÿæˆå‡ºé”™: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›æˆªæ–­ç‰ˆæœ¬
            truncated = text[:max_len] + "..." if len(text) > max_len else text
            return {
                'summary': truncated,
                'original_length': len(text),
                'summary_length': len(truncated),
                'compression_ratio': len(truncated) / len(text),
                'method': 'truncation',
                'error': f'æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›æˆªæ–­ç‰ˆæœ¬: {str(e)}'
            }

    def _llm_summarize(self, query: str, text: str, max_length: int,
                       style: str) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦
        
        Args:
            query: æŸ¥è¯¢
            text: åŸæ–‡
            max_length: æœ€å¤§é•¿åº¦
            style: æ‘˜è¦é£æ ¼
            
        Returns:
            LLMç”Ÿæˆçš„æ‘˜è¦
        """
        style_prompts = {
            'general': 'è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯å’Œå…³é”®ç‚¹ï¼š',
            'academic': 'è¯·å¯¹ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼Œé‡ç‚¹ä¿ç•™ç ”ç©¶æ–¹æ³•ã€å‘ç°å’Œç»“è®ºï¼š',
            'news': 'è¯·å¯¹ä»¥ä¸‹æ–°é—»æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼Œçªå‡ºå…³é”®äº‹å®å’Œé‡è¦ç»†èŠ‚ï¼š',
            'bullet_points': 'è¯·ç”¨è¦ç‚¹å½¢å¼æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦å†…å®¹ï¼š'
        }
        
        prompt = style_prompts.get(style, style_prompts['general'])
        prompt += f"\n\næŸ¥è¯¢å†…å®¹ï¼š{query}\n\nåŸæ–‡ï¼š\n{text}\n\nè¦æ±‚ï¼š\n1. é‡ç‚¹æ€»ç»“ä¸æŸ¥è¯¢å†…å®¹ã€Œ{query}ã€æœ€ç›¸å…³çš„ä¿¡æ¯\n2. ä¼˜å…ˆæå–èƒ½å›ç­”æŸ¥è¯¢çš„å…³é”®å†…å®¹å’Œç»†èŠ‚\n3. ä¸¥æ ¼æ§åˆ¶æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡{max_length}å­—ç¬¦\n4. ä¿æŒç›¸å…³ä¿¡æ¯çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§\n5. è¯­è¨€ç®€æ´æ¸…æ™°ï¼Œç›´æ¥ç”Ÿæˆæ‘˜è¦å†…å®¹ï¼Œä¸è¦ç”Ÿæˆå…¶ä»–æ²¡æœ‰ç”¨çš„å†…å®¹"
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ‘˜è¦ä¸“å®¶ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = self.llm_client.chat.completions.create(
                model='deepseek-chat',  # ä½¿ç”¨v3æ¨¡å‹
                messages=messages,
                temperature=0.3,
                max_tokens=1024,
                stream=False,
            )
            summary = response.choices[0].message.content.strip()
            # print(f"æ‘˜è¦å†…å®¹:\n{summary[:200]}...")  # åªæ‰“å°å‰200å­—ç¬¦
            
            # å¦‚æœæ‘˜è¦ä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­
            if len(summary) > max_length:
                summary = summary[:max_length].rsplit('ã€‚', 1)[0] + 'ã€‚'
            
            return summary
        except Exception as e:
            print(f"âŒ LLMæ‘˜è¦å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°æŠ½å–å¼æ‘˜è¦")
            # LLMå¤±è´¥æ—¶å›é€€åˆ°æŠ½å–å¼æ‘˜è¦
            return self._extractive_summarize(text, max_length)
    
    def _extractive_summarize(self, text: str, max_length: int) -> str:
        """
        æŠ½å–å¼æ‘˜è¦ï¼ˆåŸºäºè§„åˆ™ï¼‰
        
        Args:
            text: åŸæ–‡
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æŠ½å–å¼æ‘˜è¦
        """
        # åˆ†å‰²æˆå¥å­
        sentences = self._split_sentences(text)
        
        if not sentences:
            return text[:max_length]
        
        # è®¡ç®—å¥å­é‡è¦æ€§åˆ†æ•°
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = self._calculate_sentence_score(sentence, i, len(sentences))
            scored_sentences.append((sentence, score, i))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©æœ€é‡è¦çš„å¥å­ï¼Œç›´åˆ°è¾¾åˆ°é•¿åº¦é™åˆ¶
        selected_sentences = []
        current_length = 0
        
        for sentence, score, original_index in scored_sentences:
            if current_length + len(sentence) <= max_length:
                selected_sentences.append((sentence, original_index))
                current_length += len(sentence)
            else:
                break
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        selected_sentences.sort(key=lambda x: x[1])
        
        # ç»„åˆæ‘˜è¦
        summary = ''.join([sentence for sentence, _ in selected_sentences])
        
        return summary.strip()
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆåŸºäºæ ‡ç‚¹ç¬¦å·ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]+', text)
        
        # æ¸…ç†å’Œè¿‡æ»¤
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„å¥å­
                cleaned_sentences.append(sentence + 'ã€‚')
        
        return cleaned_sentences
    
    def _calculate_sentence_score(self, sentence: str, position: int, 
                                total_sentences: int) -> float:
        """
        è®¡ç®—å¥å­é‡è¦æ€§åˆ†æ•°
        
        Args:
            sentence: å¥å­æ–‡æœ¬
            position: å¥å­åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
            total_sentences: æ€»å¥å­æ•°
            
        Returns:
            é‡è¦æ€§åˆ†æ•°
        """
        score = 0.0
        
        # ä½ç½®ç‰¹å¾ï¼šå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦
        if position == 0:
            score += 0.3
        elif position == total_sentences - 1:
            score += 0.2
        elif position < total_sentences * 0.2:
            score += 0.1
        
        # é•¿åº¦ç‰¹å¾ï¼šé€‚ä¸­é•¿åº¦çš„å¥å­æ›´é‡è¦
        sentence_len = len(sentence)
        if 20 <= sentence_len <= 100:
            score += 0.2
        elif sentence_len > 100:
            score += 0.1
        
        # å…³é”®è¯ç‰¹å¾
        important_words = [
            'é‡è¦', 'å…³é”®', 'ä¸»è¦', 'æ ¸å¿ƒ', 'åŸºæœ¬', 'æ˜¾è‘—', 'æ˜æ˜¾',
            'ç ”ç©¶', 'å‘ç°', 'ç»“æœ', 'ç»“è®º', 'æ–¹æ³•', 'åˆ†æ',
            'å› æ­¤', 'æ‰€ä»¥', 'æ€»ä¹‹', 'ç»¼ä¸Š', 'å¯è§'
        ]
        
        for word in important_words:
            if word in sentence:
                score += 0.1
        
        # æ•°å­—å’Œç»Ÿè®¡æ•°æ®
        if re.search(r'\d+', sentence):
            score += 0.05
        
        # å¼•ç”¨å’Œä¸“æœ‰åè¯
        if re.search(r'[A-Z][a-z]+|ã€Š[^ã€‹]+ã€‹', sentence):
            score += 0.05
        
        return score
    
    def batch_summarize(self, query: str, text: str, 
                       chunk_size: int = 5000, 
                       chunk_summary_length: int = 300,
                       final_summary_length: int = 1000,
                       style: str = "general") -> Dict[str, Any]:
        """
        åˆ†æ‰¹æ€»ç»“é•¿æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            text: å¾…æ€»ç»“çš„æ–‡æœ¬
            chunk_size: æ¯æ‰¹å¤„ç†çš„å­—ç¬¦æ•° (é»˜è®¤5000)
            chunk_summary_length: æ¯æ‰¹æ€»ç»“çš„é•¿åº¦ (é»˜è®¤300å­—ç¬¦)
            final_summary_length: æœ€ç»ˆæ€»ç»“çš„é•¿åº¦ (é»˜è®¤1000å­—ç¬¦)
            style: æ€»ç»“é£æ ¼
            
        Returns:
            æ€»ç»“ç»“æœå­—å…¸
        """
        if not text or not text.strip():
            return {
                'summary': '',
                'original_length': 0,
                'summary_length': 0,
                'compression_ratio': 0,
                'method': 'empty',
                'chunks_processed': 0,
                'error': 'è¾“å…¥æ–‡æœ¬ä¸ºç©º'
            }
        
        try:
            print(f"ğŸ“ å¼€å§‹åˆ†æ‰¹æ€»ç»“ï¼ŒåŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
            print(f"ğŸ“¦ åˆ†æ‰¹å‚æ•°: å—å¤§å°={chunk_size}, å—æ€»ç»“é•¿åº¦={chunk_summary_length}, æœ€ç»ˆé•¿åº¦={final_summary_length}")
            
            # åˆ†å‰²æ–‡æœ¬ä¸ºå—
            chunks = self._split_text_into_chunks(text, chunk_size)
            print(f"ğŸ“¦ æ–‡æœ¬åˆ†å‰²ä¸º {len(chunks)} å—")
            
            # å¯¹æ¯ä¸ªå—è¿›è¡Œæ€»ç»“
            chunk_summaries = []
            for i, chunk in enumerate(chunks):
                print(f"ğŸ“ æ­£åœ¨æ€»ç»“ç¬¬ {i+1}/{len(chunks)} å— (é•¿åº¦: {len(chunk)} å­—ç¬¦)")
                
                if self.llm_client:
                    chunk_summary = self._llm_summarize(query, chunk, chunk_summary_length, style)
                else:
                    chunk_summary = self._extractive_summarize(chunk, chunk_summary_length)
                
                chunk_summaries.append(chunk_summary)
                print(f"âœ… ç¬¬ {i+1} å—æ€»ç»“å®Œæˆ (é•¿åº¦: {len(chunk_summary)} å­—ç¬¦)")
            
            # åˆå¹¶æ‰€æœ‰å—çš„æ€»ç»“
            combined_summary = "\n\n".join(chunk_summaries)
            print(f"ğŸ”— åˆå¹¶æ‰€æœ‰å—æ€»ç»“ï¼Œæ€»é•¿åº¦: {len(combined_summary)} å­—ç¬¦")
            
            # å¯¹åˆå¹¶åçš„æ€»ç»“è¿›è¡Œæœ€ç»ˆæ€»ç»“
            if len(combined_summary) <= final_summary_length:
                print("ğŸ“ åˆå¹¶æ€»ç»“å·²ç¬¦åˆé•¿åº¦è¦æ±‚ï¼Œæ— éœ€å†æ¬¡æ€»ç»“")
                final_summary = combined_summary
            else:
                print("ğŸ“ å¯¹åˆå¹¶æ€»ç»“è¿›è¡Œæœ€ç»ˆæ€»ç»“")
                if self.llm_client:
                    final_summary = self._llm_summarize(query, combined_summary, final_summary_length, style)
                else:
                    final_summary = self._extractive_summarize(combined_summary, final_summary_length)
            
            compression_ratio = len(final_summary) / len(text)
            
            print(f"âœ… åˆ†æ‰¹æ€»ç»“å®Œæˆï¼Œæœ€ç»ˆå‹ç¼©æ¯”: {compression_ratio:.3f}")
            
            return final_summary
            
        except Exception as e:
            print(f"âŒ åˆ†æ‰¹æ€»ç»“å‡ºé”™: {str(e)}")
            # å‡ºé”™æ—¶å›é€€åˆ°å¸¸è§„æ€»ç»“
            print("ğŸ”„ å›é€€åˆ°å¸¸è§„æ€»ç»“")
            return self.summarize(text, final_summary_length, style)
    
    def _split_text_into_chunks(self, text: str, chunk_size: int) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºæŒ‡å®šå¤§å°çš„å—ï¼Œå°½é‡åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
        
        Args:
            text: åŸæ–‡æœ¬
            chunk_size: æ¯å—çš„ç›®æ ‡å¤§å°
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        current_chunk = ""
        
        # å…ˆå°è¯•æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        for paragraph in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šè¿™ä¸ªæ®µè½ä¸ä¼šè¶…å‡ºå¤§å°é™åˆ¶
            if len(current_chunk) + len(paragraph) + 2 <= chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # å¦‚æœå½“å‰å—ä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = ""
                
                # å¦‚æœæ®µè½æœ¬èº«å°±è¶…è¿‡å—å¤§å°ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(paragraph) > chunk_size:
                    # æŒ‰å¥å­åˆ†å‰²æ®µè½
                    sentences = self._split_sentences(paragraph)
                    current_sentence_chunk = ""
                    
                    for sentence in sentences:
                        if len(current_sentence_chunk) + len(sentence) <= chunk_size:
                            current_sentence_chunk += sentence
                        else:
                            if current_sentence_chunk:
                                chunks.append(current_sentence_chunk)
                            current_sentence_chunk = sentence
                    
                    if current_sentence_chunk:
                        current_chunk = current_sentence_chunk
                else:
                    current_chunk = paragraph
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
