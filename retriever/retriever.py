"""
å®æ—¶å‘é‡æ£€ç´¢å™¨ï¼šquery->document
"""
import os
import pickle
import json
from typing import List, Dict, Any, Optional, Tuple
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import Config


class VectorRetriever:
    """å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        self.config = Config()
        self.embedder = None
        self.index = None
        self.documents = []
        self.index_type = 'simple'
        self._initialize_components()
        self._load_index()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            from retriever.embedder import JinaEmbedder
            self.embedder = JinaEmbedder()
        except ImportError as e:
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥åµŒå…¥å™¨ - {e}")
            # ä½¿ç”¨å¤‡ç”¨åµŒå…¥å™¨
            try:
                from retriever.embedder import LocalEmbedder
                self.embedder = LocalEmbedder()
            except ImportError:
                print("âŒ æ— æ³•å¯¼å…¥ä»»ä½•åµŒå…¥å™¨")
    
    def _load_index(self):
        """åŠ è½½ç´¢å¼•"""
        try:
            print("ğŸ“‚ åŠ è½½ç´¢å¼•...")
            
            # åŠ è½½æ–‡æ¡£
            docs_path = os.path.join(Config.INDEX_DIR, 'documents.pkl')
            if os.path.exists(docs_path):
                with open(docs_path, 'rb') as f:
                    self.documents = pickle.load(f)
                print(f"ğŸ“„ åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")
            else:
                print("âš ï¸ æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºç©ºç´¢å¼•")
                self.documents = []
                return
            
            # å°è¯•åŠ è½½FAISSç´¢å¼•
            faiss_path = Config.FAISS_INDEX_PATH
            if os.path.exists(faiss_path):
                try:
                    import faiss
                    self.index = faiss.read_index(faiss_path)
                    self.index_type = 'faiss'
                    print("âœ… FAISSç´¢å¼•åŠ è½½æˆåŠŸ")
                    return
                except ImportError:
                    print("âš ï¸ FAISSæœªå®‰è£…ï¼Œå°è¯•åŠ è½½ç®€å•ç´¢å¼•")
                except Exception as e:
                    print(f"âš ï¸ FAISSç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
            
            # å°è¯•åŠ è½½ç®€å•ç´¢å¼•
            simple_path = os.path.join(Config.INDEX_DIR, 'simple_index.pkl')
            if os.path.exists(simple_path):
                with open(simple_path, 'rb') as f:
                    self.index = pickle.load(f)
                self.index_type = 'simple'
                print("âœ… ç®€å•ç´¢å¼•åŠ è½½æˆåŠŸ")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç´¢å¼•æ–‡ä»¶")
                self.index = None
                
        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
            self.index = None
            self.documents = []
    
    async def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.embedder:
            return [{
                'content': 'æ£€ç´¢å™¨æœªæ­£ç¡®åˆå§‹åŒ–',
                'source': 'retriever_error',
                'score': 0.0,
                'error': True
            }]
        
        if not self.documents:
            return [{
                'content': 'çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ„å»ºç´¢å¼•',
                'source': 'empty_knowledge_base',
                'score': 0.0,
                'error': True
            }]
        
        try:
            print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = await self.embedder.embed_single(query)
            
            if self.index_type == 'faiss' and self.index:
                results = await self._search_faiss(query_embedding, top_k)
            else:
                results = await self._search_simple(query_embedding, top_k)
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            return [{
                'content': f'æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}',
                'source': 'search_error',
                'score': 0.0,
                'error': True
            }]
    
    async def _search_faiss(self, query_embedding: List[float], 
                          top_k: int) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨FAISSæœç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢åµŒå…¥
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            import numpy as np
            
            # è½¬æ¢æŸ¥è¯¢åµŒå…¥
            query_vector = np.array([query_embedding], dtype=np.float32)
            
            # æœç´¢
            scores, indices = self.index.search(query_vector, min(top_k, len(self.documents)))
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx >= 0 and idx < len(self.documents):
                    doc = self.documents[idx]
                    
                    # FAISSè¿”å›çš„æ˜¯L2è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    similarity = 1.0 / (1.0 + float(score))
                    
                    result = {
                        'content': doc.get('content', ''),
                        'title': doc.get('title', ''),
                        'source': doc.get('source', ''),
                        'score': similarity,
                        'metadata': doc.get('metadata', {}),
                        'id': doc.get('id', str(idx))
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ FAISSæœç´¢å¤±è´¥: {str(e)}")
            return await self._search_simple(query_embedding, top_k)
    
    async def _search_simple(self, query_embedding: List[float], 
                           top_k: int) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ç®€å•æœç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢åµŒå…¥
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            if self.index and 'embeddings' in self.index:
                doc_embeddings = self.index['embeddings']
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„åµŒå…¥ï¼Œç°åœºè®¡ç®—
                texts = []
                for doc in self.documents:
                    text = doc.get('content', '')
                    if doc.get('title'):
                        text = doc['title'] + '\n' + text
                    texts.append(text)
                
                doc_embeddings = await self.embedder.embed_texts(texts)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = self.embedder.batch_similarity(query_embedding, doc_embeddings)
            
            # æ’åºå¹¶è·å–top-k
            scored_docs = list(zip(self.documents, similarities, range(len(self.documents))))
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            results = []
            for doc, score, idx in scored_docs[:top_k]:
                result = {
                    'content': doc.get('content', ''),
                    'title': doc.get('title', ''),
                    'source': doc.get('source', ''),
                    'score': float(score),
                    'metadata': doc.get('metadata', {}),
                    'id': doc.get('id', str(idx))
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ ç®€å•æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def search_similar(self, document_id: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢ä¸æŒ‡å®šæ–‡æ¡£ç›¸ä¼¼çš„æ–‡æ¡£
        
        Args:
            document_id: æ–‡æ¡£ID
            top_k: è¿”å›æ•°é‡
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # æ‰¾åˆ°ç›®æ ‡æ–‡æ¡£
            target_doc = None
            target_idx = None
            
            for i, doc in enumerate(self.documents):
                if doc.get('id') == document_id:
                    target_doc = doc
                    target_idx = i
                    break
            
            if not target_doc:
                return []
            
            # è·å–ç›®æ ‡æ–‡æ¡£çš„åµŒå…¥
            if self.index and 'embeddings' in self.index:
                doc_embeddings = self.index['embeddings']
                target_embedding = doc_embeddings[target_idx]
            else:
                target_text = target_doc.get('content', '')
                if target_doc.get('title'):
                    target_text = target_doc['title'] + '\n' + target_text
                target_embedding = await self.embedder.embed_single(target_text)
            
            # ä½¿ç”¨ç›®æ ‡æ–‡æ¡£çš„åµŒå…¥è¿›è¡Œæœç´¢
            results = await self._search_simple(target_embedding, top_k + 1)
            
            # è¿‡æ»¤æ‰ç›®æ ‡æ–‡æ¡£æœ¬èº«
            filtered_results = [r for r in results if r.get('id') != document_id]
            
            return filtered_results[:top_k]
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def get_document_by_id(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–æ–‡æ¡£
        
        Args:
            document_id: æ–‡æ¡£ID
            
        Returns:
            æ–‡æ¡£å­—å…¸
        """
        for doc in self.documents:
            if doc.get('id') == document_id:
                return doc
        return None
    
    def get_documents_by_source(self, source_pattern: str) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ¥æºæ¨¡å¼è·å–æ–‡æ¡£
        
        Args:
            source_pattern: æ¥æºæ¨¡å¼
            
        Returns:
            åŒ¹é…çš„æ–‡æ¡£åˆ—è¡¨
        """
        matching_docs = []
        for doc in self.documents:
            source = doc.get('source', '')
            if source_pattern in source:
                matching_docs.append(doc)
        return matching_docs
    
    async def hybrid_search(self, query: str, top_k: int = 10, 
                          keyword_weight: float = 0.3) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼ˆè¯­ä¹‰æœç´¢ + å…³é”®è¯æœç´¢ï¼‰
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›æ•°é‡
            keyword_weight: å…³é”®è¯æƒé‡
            
        Returns:
            æ··åˆæœç´¢ç»“æœ
        """
        try:
            # è¯­ä¹‰æœç´¢
            semantic_results = await self.search(query, top_k * 2)
            
            # å…³é”®è¯æœç´¢
            keyword_results = self._keyword_search(query, top_k * 2)
            
            # åˆå¹¶å’Œé‡æ–°è¯„åˆ†
            combined_results = self._combine_search_results(
                semantic_results, keyword_results, keyword_weight
            )
            
            return combined_results[:top_k]
            
        except Exception as e:
            print(f"âŒ æ··åˆæœç´¢å¤±è´¥: {str(e)}")
            return await self.search(query, top_k)
    
    def _keyword_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """
        å…³é”®è¯æœç´¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›æ•°é‡
            
        Returns:
            å…³é”®è¯æœç´¢ç»“æœ
        """
        query_words = query.lower().split()
        scored_docs = []
        
        for i, doc in enumerate(self.documents):
            content = doc.get('content', '').lower()
            title = doc.get('title', '').lower()
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            content_score = sum(content.count(word) for word in query_words)
            title_score = sum(title.count(word) * 2 for word in query_words)  # æ ‡é¢˜æƒé‡æ›´é«˜
            
            total_score = content_score + title_score
            
            if total_score > 0:
                result = {
                    'content': doc.get('content', ''),
                    'title': doc.get('title', ''),
                    'source': doc.get('source', ''),
                    'score': float(total_score),
                    'metadata': doc.get('metadata', {}),
                    'id': doc.get('id', str(i))
                }
                scored_docs.append(result)
        
        # æ’åº
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        # å½’ä¸€åŒ–åˆ†æ•°
        if scored_docs:
            max_score = scored_docs[0]['score']
            for result in scored_docs:
                result['score'] = result['score'] / max_score
        
        return scored_docs[:top_k]
    
    def _combine_search_results(self, semantic_results: List[Dict[str, Any]], 
                              keyword_results: List[Dict[str, Any]], 
                              keyword_weight: float) -> List[Dict[str, Any]]:
        """
        åˆå¹¶æœç´¢ç»“æœ
        
        Args:
            semantic_results: è¯­ä¹‰æœç´¢ç»“æœ
            keyword_results: å…³é”®è¯æœç´¢ç»“æœ
            keyword_weight: å…³é”®è¯æƒé‡
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        # åˆ›å»ºæ–‡æ¡£IDåˆ°ç»“æœçš„æ˜ å°„
        semantic_map = {r['id']: r for r in semantic_results}
        keyword_map = {r['id']: r for r in keyword_results}
        
        all_doc_ids = set(semantic_map.keys()) | set(keyword_map.keys())
        
        combined_results = []
        for doc_id in all_doc_ids:
            semantic_score = semantic_map.get(doc_id, {}).get('score', 0.0)
            keyword_score = keyword_map.get(doc_id, {}).get('score', 0.0)
            
            # åŠ æƒåˆå¹¶åˆ†æ•°
            combined_score = (1 - keyword_weight) * semantic_score + keyword_weight * keyword_score
            
            # ä½¿ç”¨è¯­ä¹‰æœç´¢çš„ç»“æœä½œä¸ºåŸºç¡€ï¼ˆé€šå¸¸è´¨é‡æ›´é«˜ï¼‰
            if doc_id in semantic_map:
                result = semantic_map[doc_id].copy()
            else:
                result = keyword_map[doc_id].copy()
            
            result['score'] = combined_score
            result['semantic_score'] = semantic_score
            result['keyword_score'] = keyword_score
            
            combined_results.append(result)
        
        # æŒ‰åˆå¹¶åˆ†æ•°æ’åº
        combined_results.sort(key=lambda x: x['score'], reverse=True)
        
        return combined_results
    
    def get_retriever_info(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢å™¨ä¿¡æ¯"""
        return {
            'num_documents': len(self.documents),
            'index_type': self.index_type,
            'embedder_available': self.embedder is not None,
            'index_loaded': self.index is not None,
            'capabilities': [
                'è¯­ä¹‰æœç´¢',
                'å…³é”®è¯æœç´¢',
                'æ··åˆæœç´¢',
                'ç›¸ä¼¼æ–‡æ¡£æ¨è',
                'å¤šæ¨¡å¼æ£€ç´¢'
            ]
        }
