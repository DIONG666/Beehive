"""
åŸºäºæ–°ç³»ç»Ÿæµç¨‹çš„FRAMESè¯„ä¼°è„šæœ¬
"""
import argparse
import json
import os
import time
import asyncio
from typing import List, Dict, Any
from datetime import datetime

# å¯¼å…¥æˆ‘ä»¬çš„ç ”ç©¶ç³»ç»Ÿ
from agent.main_agent import MainAgent
from config import Config
from planner.planner import DeepSeekPlanner


class NewFramesEvaluator:
    """æ–°çš„FRAMESè¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.system = MainAgent()
        self.evaluator_planner = DeepSeekPlanner()
        self.results = []
    
    def load_existing_results(self, filename: str) -> List[Dict]:
        """åŠ è½½ç°æœ‰ç»“æœ"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return []
    
    def save_result(self, filename: str, result: Dict):
        """ä¿å­˜å•ä¸ªç»“æœ"""
        results = self.load_existing_results(filename)
        results.append(result)
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    
    def get_last_processed_index(self, results: List[Dict]) -> int:
        """è·å–æœ€åå¤„ç†çš„ç´¢å¼•"""
        if not results:
            return -1
        return max(int(r.get('index', -1)) for r in results)
    
    def generate_research_prompt(self, prompt: str, wiki_links: List[str]) -> str:
        """ç”Ÿæˆç ”ç©¶æç¤º"""
        if wiki_links:
            links_text = "\n".join(wiki_links)
            return f"æ ¹æ®ä»¥ä¸‹Wikipediaèµ„æºå›ç­”é—®é¢˜:\n{links_text}\n\né—®é¢˜: {prompt}"
        else:
            return f"é—®é¢˜: {prompt}"
    
    async def get_system_response(self, prompt: str) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå“åº”"""
        try:
            result = await self.system.execute_reasoning(prompt)
            return {
                'answer': result.get('answer', ''),
                'citations': result.get('citations', []),
                'reasoning_steps': len(result.get('reasoning_trace', [])),
                'sources_used': len(result.get('search_results', [])),
                'iterations': result.get('metadata', {}).get('iterations', 0),
                'error': result.get('metadata', {}).get('error', False)
            }
        except Exception as e:
            return {
                'answer': f'ç³»ç»Ÿé”™è¯¯: {str(e)}',
                'citations': [],
                'reasoning_steps': 0,
                'sources_used': 0,
                'iterations': 0,
                'error': True
            }
    
    async def evaluate_response(self, question: str, system_response: str, ground_truth: str) -> Dict[str, str]:
        """ä½¿ç”¨DeepSeekè¯„ä¼°å“åº”è´¨é‡"""
        evaluation_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹LLMå›ç­”çš„è´¨é‡ï¼š

é—®é¢˜: {question}

ç³»ç»Ÿå›ç­”: {system_response}

å‚è€ƒç­”æ¡ˆ: {ground_truth}

è¯„ä¼°æ ‡å‡†ï¼š
1. å›ç­”æ˜¯å¦åŒ…å«å‚è€ƒç­”æ¡ˆçš„æ ¸å¿ƒä¿¡æ¯ï¼Ÿ
2. å›ç­”æ˜¯å¦å‡†ç¡®ã€ç›¸å…³ï¼Ÿ
3. å›ç­”æ˜¯å¦æœ‰è‰¯å¥½çš„é€»è¾‘æ€§å’Œå®Œæ•´æ€§ï¼Ÿ

è¯·ç»™å‡ºè¯„ä¼°ç»“æœï¼š

è¯„ä¼°ç»“æœ: TRUE/FALSE (TRUEè¡¨ç¤ºå›ç­”è´¨é‡å¥½ï¼ŒFALSEè¡¨ç¤ºè´¨é‡å·®)
è§£é‡Š: [è¯¦ç»†è§£é‡Šè¯„ä¼°ç†ç”±]
åˆ†æ•°: [0-100çš„æ•°å­—åˆ†æ•°]
"""
        
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”æ¡ˆè´¨é‡è¯„ä¼°ä¸“å®¶ã€‚"},
                {"role": "user", "content": evaluation_prompt}
            ]
            
            response = await self.evaluator_planner.generate_response(messages)
            
            # è§£æè¯„ä¼°ç»“æœ
            lines = response.split('\n')
            decision = "FALSE"
            explanation = ""
            score = 0
            
            for line in lines:
                line = line.strip()
                if line.startswith("è¯„ä¼°ç»“æœ:"):
                    decision = "TRUE" if "TRUE" in line else "FALSE"
                elif line.startswith("è§£é‡Š:"):
                    explanation = line.split(":", 1)[1].strip()
                elif line.startswith("åˆ†æ•°:"):
                    try:
                        score = int(line.split(":", 1)[1].strip())
                    except:
                        score = 0
            
            return {
                "decision": decision,
                "explanation": explanation,
                "score": score
            }
            
        except Exception as e:
            return {
                "decision": "FALSE",
                "explanation": f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "score": 0
            }
    
    async def evaluate_dataset(self, dataset_path: str, output_file: str, max_samples: int = None):
        """è¯„ä¼°æ•°æ®é›†"""
        print(f"ğŸ“Š å¼€å§‹è¯„ä¼°æ•°æ®é›†: {dataset_path}")
        
        # åŠ è½½æ•°æ®é›†
        try:
            if dataset_path.endswith('.json'):
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    dataset = json.load(f)
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {dataset_path}")
                return
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return
        
        # æ£€æŸ¥å·²æœ‰ç»“æœ
        existing_results = self.load_existing_results(output_file)
        last_processed = self.get_last_processed_index(existing_results)
        
        print(f"ğŸ“‹ æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"ğŸ“‹ å·²å¤„ç†æ ·æœ¬: {len(existing_results)}")
        print(f"ğŸ“‹ æœ€åå¤„ç†ç´¢å¼•: {last_processed}")
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°
        if max_samples:
            dataset = dataset[:max_samples]
            print(f"ğŸ“‹ é™åˆ¶æ ·æœ¬æ•°: {max_samples}")
        
        # å¼€å§‹è¯„ä¼°
        processed_count = 0
        for i, item in enumerate(dataset):
            # è·³è¿‡å·²å¤„ç†çš„æ ·æœ¬
            if i <= last_processed:
                continue
            
            try:
                print(f"\nğŸ” å¤„ç†æ ·æœ¬ {i+1}/{len(dataset)}")
                
                # æ„å»ºç ”ç©¶æç¤º
                prompt = self.generate_research_prompt(
                    item.get('Prompt', ''), 
                    item.get('wiki_links', [])
                )
                
                # è·å–ç³»ç»Ÿå“åº”
                system_result = await self.get_system_response(prompt)
                
                # è¯„ä¼°å“åº”è´¨é‡
                evaluation = await self.evaluate_response(
                    item.get('Prompt', ''),
                    system_result['answer'],
                    item.get('Answer', '')
                )
                
                # ä¿å­˜ç»“æœ
                result = {
                    "index": i,
                    "prompt": item.get('Prompt', ''),
                    "ground_truth": item.get('Answer', ''),
                    "system_response": system_result['answer'],
                    "evaluation_decision": evaluation['decision'],
                    "evaluation_explanation": evaluation['explanation'],
                    "evaluation_score": evaluation['score'],
                    "reasoning_type": item.get('reasoning_types', 'unknown'),
                    "wiki_links": item.get('wiki_links', []),
                    "system_metadata": {
                        "citations": len(system_result['citations']),
                        "reasoning_steps": system_result['reasoning_steps'],
                        "sources_used": system_result['sources_used'],
                        "iterations": system_result['iterations'],
                        "error": system_result['error']
                    },
                    "timestamp": datetime.now().isoformat()
                }
                
                self.save_result(output_file, result)
                processed_count += 1
                
                print(f"âœ… æ ·æœ¬ {i+1} å®Œæˆ - å†³ç­–: {evaluation['decision']}, åˆ†æ•°: {evaluation['score']}")
                
                # å¯é€‰ï¼šæ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ ·æœ¬ {i+1} å¤±è´¥: {str(e)}")
                # ä¿å­˜é”™è¯¯ç»“æœ
                error_result = {
                    "index": i,
                    "prompt": item.get('Prompt', ''),
                    "ground_truth": item.get('Answer', ''),
                    "system_response": f"å¤„ç†é”™è¯¯: {str(e)}",
                    "evaluation_decision": "FALSE",
                    "evaluation_explanation": f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}",
                    "evaluation_score": 0,
                    "reasoning_type": item.get('reasoning_types', 'unknown'),
                    "error": True,
                    "timestamp": datetime.now().isoformat()
                }
                self.save_result(output_file, error_result)
                continue
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼å¤„ç†äº† {processed_count} ä¸ªæ–°æ ·æœ¬")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        await self.generate_report(output_file)
    
    async def generate_report(self, results_file: str):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        results = self.load_existing_results(results_file)
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœ")
            return
        
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(results)
        correct_answers = sum(1 for r in results if r['evaluation_decision'] == 'TRUE')
        accuracy = correct_answers / total_samples if total_samples > 0 else 0
        
        # åˆ†æ•°ç»Ÿè®¡
        scores = [r.get('evaluation_score', 0) for r in results]
        avg_score = sum(scores) / len(scores) if scores else 0
        
        # ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
        avg_iterations = sum(r.get('system_metadata', {}).get('iterations', 0) for r in results) / total_samples
        avg_sources = sum(r.get('system_metadata', {}).get('sources_used', 0) for r in results) / total_samples
        error_rate = sum(1 for r in results if r.get('system_metadata', {}).get('error', False)) / total_samples
        
        print(f"\nğŸ“ˆ è¯„ä¼°æŠ¥å‘Š")
        print(f"=" * 50)
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"æ­£ç¡®ç­”æ¡ˆæ•°: {correct_answers}")
        print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"å¹³å‡åˆ†æ•°: {avg_score:.1f}/100")
        print(f"å¹³å‡è¿­ä»£æ¬¡æ•°: {avg_iterations:.1f}")
        print(f"å¹³å‡ä½¿ç”¨æ¥æºæ•°: {avg_sources:.1f}")
        print(f"é”™è¯¯ç‡: {error_rate:.2%}")
        
        # æŒ‰æ¨ç†ç±»å‹åˆ†æ
        reasoning_types = {}
        for r in results:
            rt = r.get('reasoning_type', 'unknown')
            if rt not in reasoning_types:
                reasoning_types[rt] = {'total': 0, 'correct': 0, 'scores': []}
            reasoning_types[rt]['total'] += 1
            if r['evaluation_decision'] == 'TRUE':
                reasoning_types[rt]['correct'] += 1
            reasoning_types[rt]['scores'].append(r.get('evaluation_score', 0))
        
        print(f"\nğŸ“Š æŒ‰æ¨ç†ç±»å‹åˆ†æ:")
        for rt, stats in reasoning_types.items():
            accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            avg_score = sum(stats['scores']) / len(stats['scores']) if stats['scores'] else 0
            print(f"  {rt}: {accuracy:.2%} ({stats['correct']}/{stats['total']}) - å¹³å‡åˆ†: {avg_score:.1f}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ–°ç³»ç»ŸFRAMESè¯„ä¼°")
    parser.add_argument("--dataset", required=True, help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--output", default="evaluation_results_new.json", help="ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--max-samples", type=int, help="æœ€å¤§æ ·æœ¬æ•°é™åˆ¶")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®
    config = Config()
    if not config.DEEPSEEK_API_KEY:
        print("âŒ é”™è¯¯: DEEPSEEK_API_KEY æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY='your_key'")
        return 1
    
    # å¼€å§‹è¯„ä¼°
    evaluator = NewFramesEvaluator()
    
    try:
        await evaluator.evaluate_dataset(
            args.dataset,
            args.output,
            args.max_samples
        )
        return 0
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
