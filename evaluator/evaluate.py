"""
è¯„æµ‹è„šæœ¬ï¼šå…¼å®¹FRAMESè¯„æµ‹ä»£ç æ ¼å¼
"""
import json
import os
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import Config


class FramesEvaluator:
    """FRAMESåŸºå‡†è¯„æµ‹å™¨"""
    
    def __init__(self, research_system):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            research_system: ç ”ç©¶ç³»ç»Ÿå®žä¾‹
        """
        self.research_system = research_system
        self.config = Config()
        self.results = []
        self.metrics = {
            'accuracy': 0.0,
            'exact_match': 0.0,
            'f1_score': 0.0,
            'citation_accuracy': 0.0,
            'average_response_time': 0.0
        }
    
    async def evaluate(self, dataset_path: str) -> Dict[str, float]:
        """
        è¯„æµ‹ç³»ç»Ÿåœ¨FRAMESæ•°æ®é›†ä¸Šçš„è¡¨çŽ°
        
        Args:
            dataset_path: FRAMESæ•°æ®é›†è·¯å¾„
            
        Returns:
            è¯„æµ‹ç»“æžœå­—å…¸
        """
        try:
            print(f"ðŸ§ª å¼€å§‹FRAMESåŸºå‡†è¯„æµ‹...")
            print(f"ðŸ“ æ•°æ®é›†è·¯å¾„: {dataset_path}")
            
            # åŠ è½½æ•°æ®é›†
            dataset = await self._load_frames_dataset(dataset_path)
            
            if not dataset:
                print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥")
                return {}
            
            print(f"ðŸ“Š æ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªé—®é¢˜")
            
            # æ‰§è¡Œè¯„æµ‹
            self.results = []
            
            for i, item in enumerate(dataset):
                print(f"\nðŸ” å¤„ç†é—®é¢˜ {i+1}/{len(dataset)}")
                result = await self._evaluate_single_item(item, i+1)
                self.results.append(result)
                
                # æ¯10ä¸ªé—®é¢˜è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 10 == 0:
                    current_accuracy = self._calculate_current_accuracy()
                    print(f"ðŸ“ˆ å½“å‰å‡†ç¡®çŽ‡: {current_accuracy:.2%}")
            
            # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
            final_metrics = self._calculate_metrics()
            
            # ä¿å­˜ç»“æžœ
            await self._save_evaluation_results(dataset_path, final_metrics)
            
            print(f"\nðŸŽ¯ è¯„æµ‹å®Œæˆ!")
            self._print_final_results(final_metrics)
            
            return final_metrics
            
        except Exception as e:
            print(f"âŒ è¯„æµ‹è¿‡ç¨‹å¤±è´¥: {str(e)}")
            return {}
    
    async def _load_frames_dataset(self, dataset_path: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½FRAMESæ•°æ®é›†
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            
        Returns:
            æ•°æ®é›†åˆ—è¡¨
        """
        try:
            if os.path.isfile(dataset_path):
                # å•ä¸ªJSONæ–‡ä»¶
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict) and 'questions' in data:
                    return data['questions']
                else:
                    print("âŒ ä¸æ”¯æŒçš„æ•°æ®é›†æ ¼å¼")
                    return []
            
            elif os.path.isdir(dataset_path):
                # ç›®å½•ä¸­çš„å¤šä¸ªæ–‡ä»¶
                dataset = []
                for file_name in os.listdir(dataset_path):
                    if file_name.endswith('.json'):
                        file_path = os.path.join(dataset_path, file_name)
                        with open(file_path, 'r', encoding='utf-8') as f:
                            file_data = json.load(f)
                            if isinstance(file_data, list):
                                dataset.extend(file_data)
                            elif isinstance(file_data, dict):
                                dataset.append(file_data)
                
                return dataset
            
            else:
                print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
                return []
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []
    
    async def _evaluate_single_item(self, item: Dict[str, Any], item_number: int) -> Dict[str, Any]:
        """
        è¯„æµ‹å•ä¸ªé—®é¢˜
        
        Args:
            item: æ•°æ®é›†é¡¹ç›®
            item_number: é¡¹ç›®ç¼–å·
            
        Returns:
            è¯„æµ‹ç»“æžœ
        """
        try:
            # æå–é—®é¢˜å’Œç­”æ¡ˆ
            question = item.get('question', item.get('query', ''))
            expected_answer = item.get('answer', item.get('expected_answer', ''))
            expected_citations = item.get('citations', item.get('references', []))
            context = item.get('context', '')
            
            if not question:
                return {
                    'item_number': item_number,
                    'question': '',
                    'error': 'Question not found in item',
                    'accuracy': 0.0,
                    'exact_match': 0.0,
                    'f1_score': 0.0,
                    'citation_accuracy': 0.0,
                    'response_time': 0.0
                }
            
            print(f"â“ é—®é¢˜: {question[:100]}...")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = asyncio.get_event_loop().time()
            
            # è°ƒç”¨ç ”ç©¶ç³»ç»Ÿ
            try:
                system_result = await self.research_system.research_query(question, context)
            except Exception as e:
                print(f"âŒ ç³»ç»Ÿè°ƒç”¨å¤±è´¥: {str(e)}")
                return {
                    'item_number': item_number,
                    'question': question,
                    'error': f'System error: {str(e)}',
                    'accuracy': 0.0,
                    'exact_match': 0.0,
                    'f1_score': 0.0,
                    'citation_accuracy': 0.0,
                    'response_time': 0.0
                }
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = asyncio.get_event_loop().time()
            response_time = end_time - start_time
            
            # æå–ç³»ç»Ÿå›žç­”å’Œå¼•ç”¨
            system_answer = system_result.get('answer', '')
            system_citations = system_result.get('citations', [])
            
            print(f"ðŸ’¡ ç³»ç»Ÿç­”æ¡ˆ: {system_answer[:100]}...")
            print(f"â±ï¸ å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            accuracy = self._calculate_answer_accuracy(system_answer, expected_answer)
            exact_match = self._calculate_exact_match(system_answer, expected_answer)
            f1_score = self._calculate_f1_score(system_answer, expected_answer)
            citation_accuracy = self._calculate_citation_accuracy(system_citations, expected_citations)
            
            return {
                'item_number': item_number,
                'question': question,
                'expected_answer': expected_answer,
                'system_answer': system_answer,
                'expected_citations': expected_citations,
                'system_citations': system_citations,
                'accuracy': accuracy,
                'exact_match': exact_match,
                'f1_score': f1_score,
                'citation_accuracy': citation_accuracy,
                'response_time': response_time,
                'reasoning_trace': system_result.get('reasoning_trace', []),
                'error': None
            }
            
        except Exception as e:
            print(f"âŒ è¯„æµ‹å•é¡¹å¤±è´¥: {str(e)}")
            return {
                'item_number': item_number,
                'question': item.get('question', ''),
                'error': str(e),
                'accuracy': 0.0,
                'exact_match': 0.0,
                'f1_score': 0.0,
                'citation_accuracy': 0.0,
                'response_time': 0.0
            }
    
    def _calculate_answer_accuracy(self, system_answer: str, expected_answer: str) -> float:
        """
        è®¡ç®—ç­”æ¡ˆå‡†ç¡®çŽ‡ï¼ˆåŸºäºŽå…³é”®è¯é‡å ï¼‰
        
        Args:
            system_answer: ç³»ç»Ÿç­”æ¡ˆ
            expected_answer: æœŸæœ›ç­”æ¡ˆ
            
        Returns:
            å‡†ç¡®çŽ‡åˆ†æ•° (0-1)
        """
        if not expected_answer or not system_answer:
            return 0.0
        
        # ç®€å•çš„è¯æ±‡é‡å è®¡ç®—
        expected_words = set(expected_answer.lower().split())
        system_words = set(system_answer.lower().split())
        
        if not expected_words:
            return 0.0
        
        overlap = expected_words & system_words
        accuracy = len(overlap) / len(expected_words)
        
        return min(accuracy, 1.0)
    
    def _calculate_exact_match(self, system_answer: str, expected_answer: str) -> float:
        """
        è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°
        
        Args:
            system_answer: ç³»ç»Ÿç­”æ¡ˆ
            expected_answer: æœŸæœ›ç­”æ¡ˆ
            
        Returns:
            ç²¾ç¡®åŒ¹é…åˆ†æ•° (0 æˆ– 1)
        """
        if not expected_answer or not system_answer:
            return 0.0
        
        # æ ‡å‡†åŒ–æ–‡æœ¬è¿›è¡Œæ¯”è¾ƒ
        normalized_system = ' '.join(system_answer.lower().split())
        normalized_expected = ' '.join(expected_answer.lower().split())
        
        return 1.0 if normalized_system == normalized_expected else 0.0
    
    def _calculate_f1_score(self, system_answer: str, expected_answer: str) -> float:
        """
        è®¡ç®—F1åˆ†æ•°
        
        Args:
            system_answer: ç³»ç»Ÿç­”æ¡ˆ
            expected_answer: æœŸæœ›ç­”æ¡ˆ
            
        Returns:
            F1åˆ†æ•°
        """
        if not expected_answer or not system_answer:
            return 0.0
        
        expected_words = set(expected_answer.lower().split())
        system_words = set(system_answer.lower().split())
        
        if not expected_words or not system_words:
            return 0.0
        
        overlap = expected_words & system_words
        
        precision = len(overlap) / len(system_words) if system_words else 0.0
        recall = len(overlap) / len(expected_words) if expected_words else 0.0
        
        if precision + recall == 0:
            return 0.0
        
        f1 = 2 * (precision * recall) / (precision + recall)
        return f1
    
    def _calculate_citation_accuracy(self, system_citations: List[str], 
                                   expected_citations: List[str]) -> float:
        """
        è®¡ç®—å¼•ç”¨å‡†ç¡®çŽ‡
        
        Args:
            system_citations: ç³»ç»Ÿå¼•ç”¨
            expected_citations: æœŸæœ›å¼•ç”¨
            
        Returns:
            å¼•ç”¨å‡†ç¡®çŽ‡
        """
        if not expected_citations:
            return 1.0 if not system_citations else 0.5  # å¦‚æžœä¸éœ€è¦å¼•ç”¨ï¼Œæ²¡æœ‰å¼•ç”¨å¾—æ»¡åˆ†
        
        if not system_citations:
            return 0.0
        
        # ç®€å•çš„å¼•ç”¨åŒ¹é…ï¼ˆåŸºäºŽURLæˆ–å…³é”®è¯ï¼‰
        matches = 0
        total_expected = len(expected_citations)
        
        for expected_cite in expected_citations:
            for system_cite in system_citations:
                if self._citations_match(expected_cite, system_cite):
                    matches += 1
                    break
        
        return matches / total_expected if total_expected > 0 else 0.0
    
    def _citations_match(self, expected: str, system: str) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªå¼•ç”¨æ˜¯å¦åŒ¹é…
        
        Args:
            expected: æœŸæœ›å¼•ç”¨
            system: ç³»ç»Ÿå¼•ç”¨
            
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        # ç®€å•çš„åŒ¹é…é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›
        expected_lower = expected.lower()
        system_lower = system.lower()
        
        # æ£€æŸ¥URLåŒ¹é…
        if 'http' in expected_lower and 'http' in system_lower:
            # æå–åŸŸåè¿›è¡Œæ¯”è¾ƒ
            expected_domain = self._extract_domain(expected)
            system_domain = self._extract_domain(system)
            return expected_domain == system_domain
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        expected_words = set(expected_lower.split())
        system_words = set(system_lower.split())
        overlap = expected_words & system_words
        
        # å¦‚æžœæœ‰è¶…è¿‡ä¸€åŠçš„è¯æ±‡é‡å ï¼Œè®¤ä¸ºåŒ¹é…
        return len(overlap) >= min(len(expected_words), len(system_words)) * 0.5
    
    def _extract_domain(self, url: str) -> str:
        """ä»ŽURLä¸­æå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except:
            return url.lower()
    
    def _calculate_current_accuracy(self) -> float:
        """è®¡ç®—å½“å‰å¹³å‡å‡†ç¡®çŽ‡"""
        if not self.results:
            return 0.0
        
        total_accuracy = sum(result.get('accuracy', 0.0) for result in self.results)
        return total_accuracy / len(self.results)
    
    def _calculate_metrics(self) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰è¯„æµ‹æŒ‡æ ‡
        
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        if not self.results:
            return self.metrics
        
        valid_results = [r for r in self.results if r.get('error') is None]
        
        if not valid_results:
            return self.metrics
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        total_accuracy = sum(r.get('accuracy', 0.0) for r in valid_results)
        total_exact_match = sum(r.get('exact_match', 0.0) for r in valid_results)
        total_f1 = sum(r.get('f1_score', 0.0) for r in valid_results)
        total_citation_accuracy = sum(r.get('citation_accuracy', 0.0) for r in valid_results)
        total_response_time = sum(r.get('response_time', 0.0) for r in valid_results)
        
        count = len(valid_results)
        
        return {
            'accuracy': total_accuracy / count,
            'exact_match': total_exact_match / count,
            'f1_score': total_f1 / count,
            'citation_accuracy': total_citation_accuracy / count,
            'average_response_time': total_response_time / count,
            'total_questions': len(self.results),
            'valid_questions': count,
            'error_rate': (len(self.results) - count) / len(self.results) if self.results else 0.0
        }
    
    async def _save_evaluation_results(self, dataset_path: str, metrics: Dict[str, float]):
        """
        ä¿å­˜è¯„æµ‹ç»“æžœ
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            metrics: è¯„æµ‹æŒ‡æ ‡
        """
        try:
            # åˆ›å»ºç»“æžœç›®å½•
            results_dir = os.path.join(Config.DATA_DIR, 'evaluation_results')
            os.makedirs(results_dir, exist_ok=True)
            
            # ç”Ÿæˆç»“æžœæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = os.path.join(results_dir, f'frames_evaluation_{timestamp}.json')
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            results_data = {
                'timestamp': datetime.now().isoformat(),
                'dataset_path': dataset_path,
                'system_info': {
                    'config': Config.get_config(),
                    'agent_info': self.research_system.main_agent.get_agent_info()
                },
                'metrics': metrics,
                'detailed_results': self.results
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            print(f"ðŸ’¾ è¯„æµ‹ç»“æžœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ä¹Ÿä¿å­˜ä¸€ä»½ç®€åŒ–çš„æ‘˜è¦
            summary_file = os.path.join(results_dir, f'frames_summary_{timestamp}.json')
            summary_data = {
                'timestamp': datetime.now().isoformat(),
                'dataset_path': dataset_path,
                'metrics': metrics
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è¯„æµ‹ç»“æžœå¤±è´¥: {str(e)}")
    
    def _print_final_results(self, metrics: Dict[str, float]):
        """
        æ‰“å°æœ€ç»ˆè¯„æµ‹ç»“æžœ
        
        Args:
            metrics: è¯„æµ‹æŒ‡æ ‡
        """
        print("\n" + "="*60)
        print("ðŸ“Š FRAMESåŸºå‡†è¯„æµ‹ç»“æžœ")
        print("="*60)
        
        print(f"æ€»é—®é¢˜æ•°: {metrics.get('total_questions', 0)}")
        print(f"æœ‰æ•ˆé—®é¢˜æ•°: {metrics.get('valid_questions', 0)}")
        print(f"é”™è¯¯çŽ‡: {metrics.get('error_rate', 0.0):.2%}")
        print()
        
        print("ðŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡:")
        print(f"  å‡†ç¡®çŽ‡ (Accuracy): {metrics.get('accuracy', 0.0):.2%}")
        print(f"  ç²¾ç¡®åŒ¹é… (Exact Match): {metrics.get('exact_match', 0.0):.2%}")
        print(f"  F1åˆ†æ•°: {metrics.get('f1_score', 0.0):.3f}")
        print(f"  å¼•ç”¨å‡†ç¡®çŽ‡: {metrics.get('citation_accuracy', 0.0):.2%}")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {metrics.get('average_response_time', 0.0):.2f}ç§’")
        
        print("\n" + "="*60)
    
    def generate_detailed_report(self) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„è¯„æµ‹æŠ¥å‘Š
        
        Returns:
            è¯„æµ‹æŠ¥å‘Šæ–‡æœ¬
        """
        if not self.results:
            return "æ— è¯„æµ‹ç»“æžœå¯ç”ŸæˆæŠ¥å‘Šã€‚"
        
        report_lines = []
        report_lines.append("# FRAMESåŸºå‡†è¯„æµ‹è¯¦ç»†æŠ¥å‘Š")
        report_lines.append(f"è¯„æµ‹æ—¶é—´: {datetime.now().isoformat()}")
        report_lines.append(f"æ€»é—®é¢˜æ•°: {len(self.results)}")
        report_lines.append("")
        
        # æŒ‰å‡†ç¡®çŽ‡åˆ†ç»„åˆ†æž
        high_accuracy = [r for r in self.results if r.get('accuracy', 0) > 0.8]
        medium_accuracy = [r for r in self.results if 0.5 <= r.get('accuracy', 0) <= 0.8]
        low_accuracy = [r for r in self.results if r.get('accuracy', 0) < 0.5]
        
        report_lines.append("## å‡†ç¡®çŽ‡åˆ†å¸ƒ")
        report_lines.append(f"é«˜å‡†ç¡®çŽ‡ (>80%): {len(high_accuracy)} é¢˜")
        report_lines.append(f"ä¸­ç­‰å‡†ç¡®çŽ‡ (50-80%): {len(medium_accuracy)} é¢˜")
        report_lines.append(f"ä½Žå‡†ç¡®çŽ‡ (<50%): {len(low_accuracy)} é¢˜")
        report_lines.append("")
        
        # é”™è¯¯åˆ†æž
        error_results = [r for r in self.results if r.get('error')]
        if error_results:
            report_lines.append("## é”™è¯¯åˆ†æž")
            for result in error_results[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                report_lines.append(f"- é—®é¢˜ {result['item_number']}: {result.get('error', '')}")
            report_lines.append("")
        
        # æ€§èƒ½åˆ†æž
        response_times = [r.get('response_time', 0) for r in self.results if r.get('response_time')]
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            max_time = max(response_times)
            min_time = min(response_times)
            
            report_lines.append("## æ€§èƒ½åˆ†æž")
            report_lines.append(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            report_lines.append(f"æœ€é•¿å“åº”æ—¶é—´: {max_time:.2f}ç§’")
            report_lines.append(f"æœ€çŸ­å“åº”æ—¶é—´: {min_time:.2f}ç§’")
            report_lines.append("")
        
        return "\n".join(report_lines)


# å‘½ä»¤è¡ŒæŽ¥å£
async def main():
    """å‘½ä»¤è¡Œè¯„æµ‹å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="FRAMESåŸºå‡†è¯„æµ‹")
    parser.add_argument("--dataset", required=True, help="FRAMESæ•°æ®é›†è·¯å¾„")
    parser.add_argument("--output", help="ç»“æžœè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # è¿™é‡Œéœ€è¦å¯¼å…¥ç ”ç©¶ç³»ç»Ÿï¼Œä½†é¿å…å¾ªçŽ¯å¯¼å…¥
    try:
        from main import MultiAgentResearchSystem
        system = MultiAgentResearchSystem()
        
        evaluator = FramesEvaluator(system)
        results = await evaluator.evaluate(args.dataset)
        
        if args.output:
            # ä¿å­˜åˆ°æŒ‡å®šç›®å½•
            os.makedirs(args.output, exist_ok=True)
            output_file = os.path.join(args.output, 'evaluation_results.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        
        return 0
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥ç ”ç©¶ç³»ç»Ÿ: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
