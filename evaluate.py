"""
FRAMESåŸºå‡†è¯„æµ‹è„šæœ¬
ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå›ç­”é—®é¢˜ï¼Œç„¶åç”¨DeepSeek-R1è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
"""
import argparse
import json
import os
import time
import io
import sys
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import Config
from openai import OpenAI
from datasets import load_dataset
from tqdm import tqdm

# é…ç½®DeepSeekå®¢æˆ·ç«¯ç”¨äºè¯„ä¼°
DEEPSEEK_CLIENT = None

def init_deepseek_client():
    """åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯"""
    global DEEPSEEK_CLIENT
    api_key = Config.DEEPSEEK_API_KEY
    if not api_key:
        raise ValueError("DEEPSEEK_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®")
    
    DEEPSEEK_CLIENT = OpenAI(
        api_key=api_key,
        base_url=Config.DEEPSEEK_BASE_URL
    )

def load_existing_results(filename: str) -> List[Dict]:
    """åŠ è½½å·²æœ‰çš„è¯„æµ‹ç»“æœ"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_result(filename: str, result: Dict):
    """ä¿å­˜å•ä¸ªè¯„æµ‹ç»“æœ"""
    results = load_existing_results(filename)
    results.append(result)
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

def get_last_processed_index(results: List[Dict]) -> int:
    """è·å–æœ€åå¤„ç†çš„ç´¢å¼•"""
    if not results:
        return -1
    return max(int(r.get('index', -1)) for r in results)

def generate_research_prompt(prompt: str, wiki_links: List[str]) -> str:
    """ç”Ÿæˆç ”ç©¶æç¤º"""
    if wiki_links:
        return f"æ ¹æ®ä»¥ä¸‹Wikipediaèµ„æºå›ç­”é—®é¢˜:\n{wiki_links}\n\né—®é¢˜: {prompt}"
    else:
        return f"é—®é¢˜: {prompt}"

def get_system_response(query: str, context: str = "") -> Dict[str, Any]:
    """
    ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè·å–å›ç­”
    
    Args:
        query: æŸ¥è¯¢é—®é¢˜
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
    Returns:
        ç³»ç»Ÿå›ç­”ç»“æœ
    """
    try:
        # å¯¼å…¥ç³»ç»Ÿ
        from main import MultiAgentResearchSystem
        system = MultiAgentResearchSystem()
        
        # è°ƒç”¨ç³»ç»Ÿ
        result =  system.research_query(query, context)
        return result
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè°ƒç”¨å¤±è´¥: {str(e)}")
        return {
            'answer': f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
            'citations': [],
            'reasoning_trace': "",
            'error': str(e)
        }

def evaluate_response_with_deepseek(question: str, system_response: str, ground_truth: str) -> Dict[str, str]:
    """
    ä½¿ç”¨DeepSeek-R1è¯„ä¼°å›ç­”çš„æ­£ç¡®æ€§
    
    Args:
        question: åŸå§‹é—®é¢˜
        system_response: ç³»ç»Ÿå›ç­”
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        
    Returns:
        è¯„ä¼°ç»“æœ (decision, explanation)
    """
    evaluation_prompt = f"""===ä»»åŠ¡===
æˆ‘éœ€è¦ä½ å¸®åŠ©è¯„ä¼°ä¸€ä¸ªAIç³»ç»Ÿæä¾›çš„ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­æ ‡å‡†ç­”æ¡ˆçš„å†…å®¹æ˜¯å¦åœ¨AIç³»ç»Ÿçš„å›ç­”ä¸­ä½“ç°ã€‚

===æŒ‡å¯¼åŸåˆ™===
1. ä»”ç»†æ¯”è¾ƒ"AIç³»ç»Ÿå›ç­”"ä¸"æ ‡å‡†ç­”æ¡ˆ"ã€‚
2. å…³æ³¨ç­”æ¡ˆçš„å®è´¨å†…å®¹ - å¯»æ‰¾ç­‰ä»·ä¿¡æ¯æˆ–æ­£ç¡®ç­”æ¡ˆã€‚
3. ä¸è¦è¿‡åˆ†å…³æ³¨ç¡®åˆ‡çš„æªè¾ï¼Œé™¤éç¡®åˆ‡æªè¾å¯¹æ„ä¹‰è‡³å…³é‡è¦ã€‚
4. ä½ çš„æœ€ç»ˆå†³å®šåº”åŸºäº"æ ‡å‡†ç­”æ¡ˆ"çš„å«ä¹‰å’Œé‡è¦äº‹å®æ˜¯å¦åœ¨"AIç³»ç»Ÿå›ç­”"ä¸­ä½“ç°ã€‚

===è¾“å…¥æ•°æ®===
- é—®é¢˜: {question}
- AIç³»ç»Ÿå›ç­”: {system_response}
- æ ‡å‡†ç­”æ¡ˆ: {ground_truth}

===è¾“å‡ºæ ¼å¼===
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¾“å‡ºæ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºå…¶å®ƒæ— å…³å†…å®¹ï¼š
<explanation>(ä½ æ˜¯å¦‚ä½•åšå‡ºå†³å®šçš„?)</explanation>
<decision>("TRUE" æˆ– "FALSE")</decision>

è¯·å¼€å§‹è¯„ä¼°ã€‚"""

    try:
        evaluation_response = DEEPSEEK_CLIENT.chat.completions.create(
            model=Config.DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå®¢è§‚å…¬æ­£çš„AIè¯„ä¼°åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": evaluation_prompt}
            ],
            max_tokens=4096,
            temperature=0.3,
        )
        
        evaluation_text = evaluation_response.choices[0].message.content.strip()
        print(f"ğŸ“‹ DeepSeekè¯„ä¼°ç»“æœ:\n{evaluation_text}")
        
        # æå–å†³å®šå’Œè§£é‡Š
        lines = evaluation_text.split('\n')
        decision = "FALSE"
        explanation = ""
        
        for line in lines:
            line = line.strip()
            if "å†³å®š:" in line or "Decision:" in line:
                decision_part = line.split(":", 1)[1].strip().upper()
                if "TRUE" in decision_part:
                    decision = "TRUE"
                elif "FALSE" in decision_part:
                    decision = "FALSE"
            elif "è§£é‡Š:" in line or "Explanation:" in line:
                explanation = line.split(":", 1)[1].strip()
        
        return {"decision": decision, "explanation": explanation}
        
    except Exception as e:
        print(f"âŒ DeepSeekè¯„ä¼°å¤±è´¥: {str(e)}")
        return {"decision": "FALSE", "explanation": f"è¯„ä¼°é”™è¯¯: {str(e)}"}

def process_single_item(item: Dict[str, Any], index: int) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªè¯„æµ‹é¡¹ç›®
    
    Args:
        item: æ•°æ®é›†é¡¹ç›®
        index: é¡¹ç›®ç´¢å¼•
        
    Returns:
        å¤„ç†ç»“æœ
    """
    # æå–é—®é¢˜ä¿¡æ¯
    question = item.get('Prompt')
    ground_truth = item.get('Answer')
    reasoning_type = item.get('reasoning_types')
    wiki_links = item.get('wiki_links')
    prompt = generate_research_prompt(question, wiki_links)

    if not question:
        return {
            'index': index,
            'error': 'Question not found in item',
            'evaluation_decision': 'FALSE'
        }
    
    print(f"ğŸ“ å¤„ç†é—®é¢˜ {index}: {question[:100]}...")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè·å–å›ç­”
    try:
        system_result =  get_system_response(prompt)
        system_answer = system_result.get('answer', '')
        system_citations = system_result.get('citations', [])
        reasoning_trace = system_result.get('reasoning_trace', "")
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè°ƒç”¨å¤±è´¥: {str(e)}")
        return {
            'index': index,
            'question': question,
            'ground_truth': ground_truth,
            'system_answer': f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
            'evaluation_decision': 'FALSE',
            'evaluation_explanation': f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
            'reasoning_type': reasoning_type,
            'response_time': 0,
            'error': str(e)
        }
    
    # è®°å½•å“åº”æ—¶é—´
    response_time = time.time() - start_time
    
    print(f"ğŸ’¡ ç³»ç»Ÿå›ç­”: {system_answer[:100]}...")
    print(f"â±ï¸ å“åº”æ—¶é—´: {response_time:.2f}ç§’")
    
    # ä½¿ç”¨DeepSeekè¯„ä¼°ç­”æ¡ˆ
    print("ğŸ” æ­£åœ¨è¯„ä¼°ç­”æ¡ˆ...")
    evaluation = evaluate_response_with_deepseek(question, system_answer, ground_truth)
    
    print(f"âœ… è¯„ä¼°ç»“æœ: {evaluation['decision']}")
    
    return {
        'index': index,
        'question': question,
        'ground_truth': ground_truth,
        'system_answer': system_answer,
        'system_citations': system_citations,
        'reasoning_trace': reasoning_trace,
        'evaluation_decision': evaluation['decision'],
        'evaluation_explanation': evaluation['explanation'],
        'reasoning_type': reasoning_type,
        'response_time': response_time,
    }

def main():
    print("ğŸš€ å¼€å§‹FRAMESåŸºå‡†è¯„æµ‹")
    print("="*60)
    
    # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
    try:
        init_deepseek_client()
        print("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return
    
    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“ åŠ è½½æ•°æ®é›†")
    dataset = load_dataset("google/frames-benchmark", split="test")
    
    if not dataset:
        print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥")
        return
    
    print(f"ğŸ“Š æ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªé—®é¢˜")
    
    # å‡†å¤‡ç»“æœæ–‡ä»¶
    filename = f"frames_evaluation_multi_agent.json"
    results_dir = os.path.join(Config.DATA_DIR, 'evaluation_results')
    os.makedirs(results_dir, exist_ok=True)
    full_filename = os.path.join(results_dir, filename)
    
    # æ£€æŸ¥å·²æœ‰ç»“æœ
    existing_results = load_existing_results(full_filename)
    last_processed_index = get_last_processed_index(existing_results)
    
    print(f"ğŸ“‹ ç»“æœå°†ä¿å­˜åˆ°: {full_filename}")
    if existing_results:
        print(f"ğŸ”„ å‘ç°å·²æœ‰ç»“æœ {len(existing_results)} ä¸ªï¼Œä»ç´¢å¼• {last_processed_index + 1} å¼€å§‹")
    
    # å¤„ç†æ•°æ®é›†
    processed_count = 0
    
    for i, item in enumerate(tqdm(dataset, desc="å¤„ç†é—®é¢˜")):
        index = i
        
        # è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®
        if index <= last_processed_index:
            continue
        
        try:
            result =  process_single_item(item, index)
            save_result(full_filename, result)
            processed_count += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜ {index} å¤±è´¥: {str(e)}")
            continue
    
    # è®¡ç®—å¹¶è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆè¯„æµ‹ç»Ÿè®¡")
    print("="*60)
    
    results = load_existing_results(full_filename)
    total_samples = len(results)
    correct_answers = sum(1 for r in results if r.get('evaluation_decision') == 'TRUE')
    accuracy = correct_answers / total_samples if total_samples > 0 else 0
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"æ­£ç¡®ç­”æ¡ˆ: {correct_answers}")
    print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
    
    # æŒ‰æ¨ç†ç±»å‹ç»Ÿè®¡å‡†ç¡®ç‡
    reasoning_types = set(r.get('reasoning_type', 'unknown') for r in results)
    print(f"\nğŸ“ˆ æŒ‰æ¨ç†ç±»å‹ç»Ÿè®¡:")
    for rt in reasoning_types:
        rt_samples = [r for r in results if r.get('reasoning_type') == rt]
        if rt_samples:
            rt_correct = sum(1 for r in rt_samples if r.get('evaluation_decision') == 'TRUE')
            rt_accuracy = rt_correct / len(rt_samples)
            print(f"  {rt}: {rt_accuracy:.2%} ({rt_correct}/{len(rt_samples)})")
    
    # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
    response_times = [r.get('response_time', 0) for r in results if r.get('response_time')]
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        print(f"\nâ±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
    
    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {full_filename}")

if __name__ == "__main__":
    # è¿è¡Œè¯„æµ‹
    main()
